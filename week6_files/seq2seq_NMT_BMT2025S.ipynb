{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNLGZS6Ii_YE"
      },
      "source": [
        "# Seq2seq NMT with RNN\n",
        "\n",
        "\n",
        "\n",
        "[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
        "\n",
        "**NOTE:**\n",
        "\n",
        "-  use clean bpe data\n",
        "-  use a piece of triaing data during coding or low in credits\n",
        "\n",
        "You have to implement:\n",
        "\n",
        "- Encoder\n",
        "- Attention (Bahdanau)\n",
        "- training loop\n",
        "- extra: BLEU model selection\n",
        "\n",
        "Goal:\n",
        "\n",
        "- Loss in training, validation and test\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxTE-EePi_YH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKtvwA_Rji-B"
      },
      "outputs": [],
      "source": [
        "#if you dont have bpe data use sacremoese tokenizer\n",
        "#!pip install sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#which libraries are we using!!?\n",
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "Xr1-PPWBrPxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat requirements.txt"
      ],
      "metadata": {
        "id": "v1iMgKrDrai0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXnagBYmi_YI"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjVU2ynri_YJ"
      },
      "outputs": [],
      "source": [
        "import torchtext\n",
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import vocab\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "import io\n",
        "\n",
        "#0=en 1=de\n",
        "# soure and target data\n",
        "#NOTE: USE clean bpe data!\n",
        "\n",
        "train_filepaths = ['train.100k.en-de.bpe.en', 'train.100k.en-de.bpe.de']\n",
        "val_filepaths = ['dev.500.en-de.bpe.en', 'dev.500.en-de.bpe.de']\n",
        "test_filepaths = ['test.500.en-de.bpe.en', 'test.500.en-de.bpe.en']\n",
        "\n",
        "\n",
        "#TODO use clean bpe tokenized data!!!\n",
        "#de_tokenizer = get_tokenizer('moses', language='de')\n",
        "de_tokenizer = None\n",
        "#en_tokenizer = get_tokenizer('moses', language='en')\n",
        "en_tokenizer = None\n",
        "\n",
        "def build_vocab(filepath, tokenizer=None):\n",
        "  counter = Counter()\n",
        "  with io.open(filepath, encoding=\"utf8\") as f:\n",
        "    for string_ in f:\n",
        "      #counter.update(tokenizer(string_))\n",
        "      counter.update(string_.split())\n",
        "  return vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "\n",
        "#Vocab\n",
        "en_vocab = build_vocab(train_filepaths[0], en_tokenizer)\n",
        "de_vocab = build_vocab(train_filepaths[1], de_tokenizer)\n",
        "\n",
        "en_vocab.set_default_index(en_vocab['<unk>'])\n",
        "de_vocab.set_default_index(de_vocab['<unk>'])\n",
        "\n",
        "def data_process(filepaths):\n",
        "  raw_en_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "  raw_de_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "  data = []\n",
        "  for (raw_en, raw_de) in zip(raw_en_iter, raw_de_iter):\n",
        "    en_tensor_ = torch.tensor([en_vocab[token] for token in raw_en.split()], #en_tokenizer(raw_en)\n",
        "                            dtype=torch.long)\n",
        "    de_tensor_ = torch.tensor([de_vocab[token] for token in raw_de.split()], #de_tokenizer(raw_de)\n",
        "                            dtype=torch.long)\n",
        "    data.append((en_tensor_, de_tensor_))\n",
        "  return data\n",
        "\n",
        "#pre-process\n",
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo6Mz24Itw7B"
      },
      "outputs": [],
      "source": [
        "#NOTE: if you are low on credits or testing only use a piece of the data eg 20K segments\n",
        "train_data = train_data[:10000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8FHO7cfuHz_",
        "outputId": "1336a5b7-c529-45ed-a0a2-0f4e21bf7a8f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "len(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyZRMk_Yi_YJ"
      },
      "source": [
        "Define the device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNS-Dmk0i_YJ",
        "outputId": "9a3d58c3-253f-4fcd-ccd7-5f2ed01009fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUIbF0Wi_YK"
      },
      "source": [
        "Create the iterators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCHlMlZYi_YK"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 8\n",
        "PAD_IDX = de_vocab['<pad>']\n",
        "BOS_IDX = de_vocab['<bos>']\n",
        "EOS_IDX = de_vocab['<eos>']\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def generate_batch(data_batch):\n",
        "    en_batch, de_batch = [], []\n",
        "    for (en_item, de_item) in data_batch:\n",
        "        de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    de_batch = pad_sequence(de_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "    return en_batch, de_batch\n",
        "\n",
        "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=False, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                       shuffle=False, collate_fn=generate_batch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjnN1Aj0i_YK"
      },
      "source": [
        "## Building the Seq2Seq Model\n",
        "\n",
        "### Encoder\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf4mEJnhi_YK"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "\n",
        "        self.rnn = #[YOUR CODE] GRU(embeding size, encoder hidden size) NOTE: bidirectional batch_first\n",
        "\n",
        "        self.fc = #[YOUR CODE] linear(encoder hidden size * 2, decoder hidden size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "\n",
        "\n",
        "        #[B, S]\n",
        "\n",
        "        embedded = self.embedding(src)\n",
        "\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "\n",
        "        #[B, S, E]\n",
        "\n",
        "        outputs, hidden = #[YOUR CODE] rrn(embeddings)\n",
        "\n",
        "\n",
        "        #[B, S, H*2]\n",
        "\n",
        "        #h[n layers * num directions, batch size, hid dim]\n",
        "\n",
        "        #[forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #\n",
        "\n",
        "        #[-2, :, : ] last state forward RNN\n",
        "        #[-1, :, : ] last state backward RNN\n",
        "        #print('hid', hidden.size())\n",
        "        h1 = #[YOUR CODE] last state forward RNN\n",
        "        h2 = #[YOUR CODE] last state backward RNN\n",
        "\n",
        "        #https://pytorch.org/docs/main/generated/torch.cat.html\n",
        "        h_cat = #[YOUR CODE] concatenate h1 amd h2 on seq dim\n",
        "\n",
        "        hidden = #[YOUR CODE] tanh(linear(hidden_cat))\n",
        "\n",
        "        #[B, S, H*2]\n",
        "\n",
        "        return outputs, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention\n",
        "\n",
        "## Luong Attention\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5Av3fvEwEAXM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP4jL01Vi_YL"
      },
      "outputs": [],
      "source": [
        "class LuongAttention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs): #keys, query\n",
        "\n",
        "        #[batch size, dec hid dim]\n",
        "        #[src len, batch size, enc hid dim * 2]\n",
        "\n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "        src_len = encoder_outputs.shape[1]\n",
        "\n",
        "        #x times decoder hidden state for src_len\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        #[batch size, src len, dec hid dim]\n",
        "\n",
        "        scores = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2)))\n",
        "\n",
        "\n",
        "        #[batch size, src len, dec hid dim]\n",
        "\n",
        "        attention = self.v(scores).squeeze(2)\n",
        "\n",
        "        #[batch size, src len]\n",
        "\n",
        "        return F.softmax(attention, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFSa__e8_ZQB"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(dec_hid_dim, dec_hid_dim, bias=False)\n",
        "        self.Ua = nn.Linear(enc_hid_dim * 2, dec_hid_dim, bias=False)\n",
        "        self.Va = nn.Linear(dec_hid_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs): #keys, query\n",
        "\n",
        "        batch_size = #[YOUR CODE]\n",
        "        src_len = #[YOUR CODE]\n",
        "\n",
        "        #x times decoder hidden state for src_len\n",
        "        hidden = #[YOUR CODE]\n",
        "\n",
        "        scores = #[YOUR CODE] Va(tanh(Wa(hidden) + Ua(encoder outputs)))\n",
        "        scores = scores.squeeze(2)\n",
        "\n",
        "        weights = #[YOUR CODE] softmax(scores, dim seq)\n",
        "\n",
        "        return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWvboaMki_YL"
      },
      "source": [
        "### Decoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_J_RZPbi_YL"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.attention = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim, batch_first=True)\n",
        "\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        #attention\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "\n",
        "        #[batch size]\n",
        "        #[batch size, dec hid dim]\n",
        "        #[src len, batch size, enc hid dim * 2]\n",
        "\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "\n",
        "        #[1, batch size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        #[1, batch size, emb dim]\n",
        "\n",
        "        a = self.attention(hidden, encoder_outputs)\n",
        "\n",
        "        #[batch size, src len]\n",
        "\n",
        "        a = a.unsqueeze(1)\n",
        "\n",
        "        #[batch size, 1, src len]\n",
        "\n",
        "        #[batch size, src len, enc hid dim * 2]\n",
        "\n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "\n",
        "        #[batch size, 1, enc hid dim * 2]\n",
        "\n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "\n",
        "        #[1, batch size, enc hid dim * 2]\n",
        "\n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "\n",
        "        #[1, batch size, (enc hid dim * 2) + emb dim]\n",
        "        #[B, 1, (enc hid dim * 2) + emb dim]\n",
        "\n",
        "        rnn_input = rnn_input.permute(1, 0, 2)\n",
        "        hidden = hidden.unsqueeze(0)\n",
        "\n",
        "        output, hidden = self.rnn(rnn_input, hidden)\n",
        "\n",
        "        #[seq len, batch size, dec hid dim * n directions]\n",
        "        #[n layers * n directions, batch size, dec hid dim]\n",
        "\n",
        "        #[1, batch size, dec hid dim]\n",
        "        #[1, batch size, dec hid dim]\n",
        "\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(1)\n",
        "        weighted = weighted.squeeze(0)\n",
        "\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "\n",
        "        #[batch size, output dim]\n",
        "\n",
        "        return prediction, hidden.squeeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d33lfUX9i_YL"
      },
      "source": [
        "### Seq2Seq\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwoMHRFxi_YL"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "\n",
        "        #[src len, batch size]\n",
        "        #[trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        # 0.75 teacher forcing 75% of the time\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
        "\n",
        "        #encoder_outputs is all hidden states of the input sequence\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[:,0]\n",
        "        # unroll RNN\n",
        "        for t in range(1, trg_len):\n",
        "\n",
        "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
        "\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "\n",
        "            #predictions\n",
        "            outputs[:, t] = output\n",
        "\n",
        "            #teacher forcing\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            #greedy search\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            #if teacher forcing, use gold token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e06yvLvei_YL"
      },
      "source": [
        "## Training the Seq2Seq Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jC9nxari_YM"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(en_vocab)\n",
        "OUTPUT_DIM = len(de_vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.2\n",
        "\n",
        "attn = LuongAttention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "#[YOUR CODE] Bahdanau att\n",
        "#attn = BahdanauAttention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGLEjaJu2NjY",
        "outputId": "2e42738c-7144-4bf3-fc4d-c164940588e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5542\n",
            "6700\n"
          ]
        }
      ],
      "source": [
        "print(len(en_vocab)) #BPE size 16k approx\n",
        "print(len(de_vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUD45Ys5i_YM",
        "outputId": "9d17b2f2-3811-4f80-f0e2-8fff16ef4545"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(5542, 256)\n",
              "    (rnn): GRU(256, 512, batch_first=True, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): LuongAttention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(6700, 256)\n",
              "    (rnn): GRU(1280, 512, batch_first=True)\n",
              "    (fc_out): Linear(in_features=1792, out_features=6700, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "    (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "    (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmY0twjqi_YM",
        "outputId": "a13f096b-9ba3-41c3-9fac-d39094065bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 22,367,788 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6M5jMx9i_YM"
      },
      "source": [
        "We create an optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkFpvqEii_YM"
      },
      "outputs": [],
      "source": [
        "optimizer = #[YOUR CODE] Adam lr 1e-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POJ6uKGei_YM"
      },
      "source": [
        "We initialize the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McD_2eGsi_YM"
      },
      "outputs": [],
      "source": [
        "TRG_PAD_IDX = de_vocab['<pad>'] #TODO\n",
        "#https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "criterion = #[YOUR CODE] loss add ignore_index idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBn2uizpi_YM",
        "outputId": "e8130dd0-336e-4c74-8ba0-efa8fd469a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "print(TRG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWBC8pDAi_YN"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "\n",
        "    #[YOUR CODE] set model train model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for (src, trg) in tqdm(iterator):\n",
        "\n",
        "        src, trg = #[YOUR CODE] to gpu\n",
        "\n",
        "        #[YOUR CODE] optimizer zero grad\n",
        "\n",
        "        output = #[YOUR CODE] model()\n",
        "\n",
        "        #[trg len, batch size]\n",
        "        #[trg len, batch size, output dim]\n",
        "\n",
        "        output = output.permute(1, 0, 2)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        trg = trg.permute(1, 0)\n",
        "\n",
        "        output = output[1:].reshape(-1, output_dim)\n",
        "        trg = trg[1:].reshape(-1)\n",
        "\n",
        "        #[(trg len - 1) * batch size]\n",
        "        #[(trg len - 1) * batch size, output dim]\n",
        "\n",
        "        loss = #[YOUR CODE] criterion()\n",
        "\n",
        "        #[YOUR CODE] loss backward\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        #[YOUR CODE] optimizer step\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJGxS6mFi_YN"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    #[YOUR CODE] set model test/eval  model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for (src, trg) in iterator:\n",
        "            src, trg = #[YOUR CODE] to device\n",
        "\n",
        "            output = #[YOUR CODE] model() turn off teacher forcing 0\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output = output.permute(1, 0, 2)\n",
        "            output_dim = output.shape[-1]\n",
        "            trg = trg.permute(1, 0)\n",
        "            output = output[1:].reshape(-1, output_dim)\n",
        "            trg = trg[1:].reshape(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = #[YOUR CODE] criterion()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFGhRjIMi_YN",
        "outputId": "1405cd9d-542b-48cb-c2d7-4c5f28fe7433"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:18<00:00,  6.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 7.785\tTrain PPL: 2405.277\n",
            "\t Validation Loss: 6.986\tValidation PPL: 1081.010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:16<00:00,  7.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02\n",
            "\tTrain Loss: 6.955\tTrain PPL: 1048.880\n",
            "\t Validation Loss: 6.758\tValidation PPL: 860.717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:16<00:00,  7.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03\n",
            "\tTrain Loss: 6.646\tTrain PPL: 769.410\n",
            "\t Validation Loss: 6.482\tValidation PPL: 653.374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:16<00:00,  7.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04\n",
            "\tTrain Loss: 6.281\tTrain PPL: 534.420\n",
            "\t Validation Loss: 6.145\tValidation PPL: 466.177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:16<00:00,  7.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05\n",
            "\tTrain Loss: 5.913\tTrain PPL: 369.934\n",
            "\t Validation Loss: 5.773\tValidation PPL: 321.486\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 5\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iter, criterion)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "      #[YOUR CODE] extra add BLEU model selection\n",
        "      best_valid_loss = valid_loss\n",
        "      #torch.save(model.state_dict(), 'model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}\\tTrain PPL: {np.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Validation Loss: {valid_loss:.3f}\\tValidation PPL: {np.exp(valid_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IiI41XDs4bj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meHcb9oOi_YN"
      },
      "outputs": [],
      "source": [
        "#NOTE: load model from file\n",
        "#model.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iter, criterion)\n",
        "\n",
        "print(f'\\tTest Loss: {test_loss:.3f}\\tTest PPL: {np.exp(test_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7dql_q_i_YN"
      },
      "outputs": [],
      "source": [
        "#clean mem\n",
        "del model\n",
        "del train_iter\n",
        "del valid_iter\n",
        "del test_iter\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z78J4LTji_YO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}