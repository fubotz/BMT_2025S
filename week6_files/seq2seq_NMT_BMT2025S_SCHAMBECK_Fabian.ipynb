{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNLGZS6Ii_YE"
      },
      "source": [
        "# Seq2seq NMT with RNN\n",
        "\n",
        "\n",
        "\n",
        "[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
        "\n",
        "**NOTE:**\n",
        "\n",
        "-  use clean bpe data\n",
        "-  use a piece of triaing data during coding or low in credits\n",
        "\n",
        "You have to implement:\n",
        "\n",
        "- Encoder\n",
        "- Attention (Bahdanau)\n",
        "- training loop\n",
        "- extra: BLEU model selection\n",
        "\n",
        "Goal:\n",
        "\n",
        "- Loss in training, validation and test\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install torch==2.3.0\n",
        "!pip install torchtext==0.18"
      ],
      "metadata": {
        "id": "FvIhSmCUcDN4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qxTE-EePi_YH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dKtvwA_Rji-B",
        "outputId": "acfc499a-684f-4403-d229-16c719656ca0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BMT_2025S'...\n",
            "remote: Enumerating objects: 376, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 376 (delta 21), reused 8 (delta 8), pack-reused 342 (from 4)\u001b[K\n",
            "Receiving objects: 100% (376/376), 90.16 MiB | 12.00 MiB/s, done.\n",
            "Resolving deltas: 100% (155/155), done.\n",
            "/content/BMT_2025S/week6_files\n"
          ]
        }
      ],
      "source": [
        "#if you dont have bpe data use sacremoese tokenizer\n",
        "#!pip install sacremoses\n",
        "\n",
        "#clone repo to access bpe files from week2_files and to put requirements.txt\n",
        "!git clone https://github.com/fubotz/BMT_2025S\n",
        "%cd BMT_2025S/week6_files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#which libraries are we using!!?\n",
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "Xr1-PPWBrPxl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat requirements.txt"
      ],
      "metadata": {
        "id": "v1iMgKrDrai0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fcd9f56-c461-456b-f322-9b11b824794b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "absl-py==1.4.0\n",
            "accelerate==1.6.0\n",
            "aiohappyeyeballs==2.6.1\n",
            "aiohttp==3.11.15\n",
            "aiosignal==1.3.2\n",
            "alabaster==1.0.0\n",
            "albucore==0.0.24\n",
            "albumentations==2.0.6\n",
            "ale-py==0.11.0\n",
            "altair==5.5.0\n",
            "annotated-types==0.7.0\n",
            "anyio==4.9.0\n",
            "argon2-cffi==23.1.0\n",
            "argon2-cffi-bindings==21.2.0\n",
            "array_record==0.7.2\n",
            "arviz==0.21.0\n",
            "astropy==7.0.1\n",
            "astropy-iers-data==0.2025.4.28.0.37.27\n",
            "astunparse==1.6.3\n",
            "atpublic==5.1\n",
            "attrs==25.3.0\n",
            "audioread==3.0.1\n",
            "autograd==1.7.0\n",
            "babel==2.17.0\n",
            "backcall==0.2.0\n",
            "backports.tarfile==1.2.0\n",
            "beautifulsoup4==4.13.4\n",
            "betterproto==2.0.0b6\n",
            "bigframes==2.1.0\n",
            "bigquery-magics==0.9.0\n",
            "bleach==6.2.0\n",
            "blinker==1.9.0\n",
            "blis==1.3.0\n",
            "blosc2==3.3.1\n",
            "bokeh==3.7.2\n",
            "Bottleneck==1.4.2\n",
            "bqplot==0.12.44\n",
            "branca==0.8.1\n",
            "build==1.2.2.post1\n",
            "CacheControl==0.14.2\n",
            "cachetools==5.5.2\n",
            "catalogue==2.0.10\n",
            "certifi==2025.4.26\n",
            "cffi==1.17.1\n",
            "chardet==5.2.0\n",
            "charset-normalizer==3.4.1\n",
            "chex==0.1.89\n",
            "clarabel==0.10.0\n",
            "click==8.1.8\n",
            "cloudpathlib==0.21.0\n",
            "cloudpickle==3.1.1\n",
            "cmake==3.31.6\n",
            "cmdstanpy==1.2.5\n",
            "colorcet==3.1.0\n",
            "colorlover==0.3.0\n",
            "colour==0.1.5\n",
            "community==1.0.0b1\n",
            "confection==0.1.5\n",
            "cons==0.4.6\n",
            "contourpy==1.3.2\n",
            "cramjam==2.10.0\n",
            "cryptography==43.0.3\n",
            "cuda-python==12.6.2.post1\n",
            "cudf-cu12 @ https://pypi.nvidia.com/cudf-cu12/cudf_cu12-25.2.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\n",
            "cudf-polars-cu12==25.2.2\n",
            "cufflinks==0.17.3\n",
            "cuml-cu12==25.2.1\n",
            "cupy-cuda12x==13.3.0\n",
            "cuvs-cu12==25.2.1\n",
            "cvxopt==1.3.2\n",
            "cvxpy==1.6.5\n",
            "cycler==0.12.1\n",
            "cyipopt==1.5.0\n",
            "cymem==2.0.11\n",
            "Cython==3.0.12\n",
            "dask==2024.12.1\n",
            "dask-cuda==25.2.0\n",
            "dask-cudf-cu12==25.2.2\n",
            "dask-expr==1.1.21\n",
            "datascience==0.17.6\n",
            "db-dtypes==1.4.2\n",
            "dbus-python==1.2.18\n",
            "debugpy==1.8.0\n",
            "decorator==4.4.2\n",
            "defusedxml==0.7.1\n",
            "Deprecated==1.2.18\n",
            "diffusers==0.33.1\n",
            "distributed==2024.12.1\n",
            "distributed-ucxx-cu12==0.42.0\n",
            "distro==1.9.0\n",
            "dlib==19.24.6\n",
            "dm-tree==0.1.9\n",
            "docker-pycreds==0.4.0\n",
            "docstring_parser==0.16\n",
            "docutils==0.21.2\n",
            "dopamine_rl==4.1.2\n",
            "duckdb==1.2.2\n",
            "earthengine-api==1.5.13\n",
            "easydict==1.13\n",
            "editdistance==0.8.1\n",
            "eerepr==0.1.1\n",
            "einops==0.8.1\n",
            "en_core_web_sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl#sha256=1932429db727d4bff3deed6b34cfc05df17794f4a52eeb26cf8928f7c1a0fb85\n",
            "entrypoints==0.4\n",
            "et_xmlfile==2.0.0\n",
            "etils==1.12.2\n",
            "etuples==0.3.9\n",
            "Farama-Notifications==0.0.4\n",
            "fastai==2.7.19\n",
            "fastcore==1.7.29\n",
            "fastdownload==0.0.7\n",
            "fastjsonschema==2.21.1\n",
            "fastprogress==1.0.3\n",
            "fastrlock==0.8.3\n",
            "filelock==3.18.0\n",
            "firebase-admin==6.8.0\n",
            "Flask==3.1.0\n",
            "flatbuffers==25.2.10\n",
            "flax==0.10.6\n",
            "folium==0.19.5\n",
            "fonttools==4.57.0\n",
            "frozendict==2.4.6\n",
            "frozenlist==1.6.0\n",
            "fsspec==2025.3.2\n",
            "future==1.0.0\n",
            "gast==0.6.0\n",
            "gcsfs==2025.3.2\n",
            "GDAL==3.6.4\n",
            "gdown==5.2.0\n",
            "geemap==0.35.3\n",
            "geocoder==1.38.1\n",
            "geographiclib==2.0\n",
            "geopandas==1.0.1\n",
            "geopy==2.4.1\n",
            "gin-config==0.5.0\n",
            "gitdb==4.0.12\n",
            "GitPython==3.1.44\n",
            "glob2==0.7\n",
            "google==2.0.3\n",
            "google-ai-generativelanguage==0.6.15\n",
            "google-api-core==2.24.2\n",
            "google-api-python-client==2.169.0\n",
            "google-auth==2.38.0\n",
            "google-auth-httplib2==0.2.0\n",
            "google-auth-oauthlib==1.2.2\n",
            "google-cloud-aiplatform==1.90.0\n",
            "google-cloud-bigquery==3.31.0\n",
            "google-cloud-bigquery-connection==1.18.2\n",
            "google-cloud-bigquery-storage==2.31.0\n",
            "google-cloud-bigtable==2.30.1\n",
            "google-cloud-core==2.4.3\n",
            "google-cloud-dataproc==5.18.1\n",
            "google-cloud-datastore==2.21.0\n",
            "google-cloud-firestore==2.20.2\n",
            "google-cloud-functions==1.20.3\n",
            "google-cloud-iam==2.19.0\n",
            "google-cloud-language==2.17.1\n",
            "google-cloud-pubsub==2.25.0\n",
            "google-cloud-resource-manager==1.14.2\n",
            "google-cloud-spanner==3.54.0\n",
            "google-cloud-storage==2.19.0\n",
            "google-cloud-translate==3.20.2\n",
            "google-colab @ file:///colabtools/dist/google_colab-1.0.0.tar.gz\n",
            "google-crc32c==1.7.1\n",
            "google-genai==1.12.1\n",
            "google-generativeai==0.8.5\n",
            "google-pasta==0.2.0\n",
            "google-resumable-media==2.7.2\n",
            "google-spark-connect==0.5.2\n",
            "googleapis-common-protos==1.70.0\n",
            "googledrivedownloader==1.1.0\n",
            "graphviz==0.20.3\n",
            "greenlet==3.2.1\n",
            "grpc-google-iam-v1==0.14.2\n",
            "grpc-interceptor==0.15.4\n",
            "grpcio==1.71.0\n",
            "grpcio-status==1.71.0\n",
            "grpclib==0.4.7\n",
            "gspread==6.2.0\n",
            "gspread-dataframe==4.0.0\n",
            "gym==0.25.2\n",
            "gym-notices==0.0.8\n",
            "gymnasium==1.1.1\n",
            "h11==0.16.0\n",
            "h2==4.2.0\n",
            "h5netcdf==1.6.1\n",
            "h5py==3.13.0\n",
            "hdbscan==0.8.40\n",
            "highspy==1.10.0\n",
            "holidays==0.71\n",
            "holoviews==1.20.2\n",
            "hpack==4.1.0\n",
            "html5lib==1.1\n",
            "httpcore==1.0.9\n",
            "httpimport==1.4.1\n",
            "httplib2==0.22.0\n",
            "httpx==0.28.1\n",
            "huggingface-hub==0.30.2\n",
            "humanize==4.12.2\n",
            "hyperframe==6.1.0\n",
            "hyperopt==0.2.7\n",
            "ibis-framework==9.5.0\n",
            "idna==3.10\n",
            "imageio==2.37.0\n",
            "imageio-ffmpeg==0.6.0\n",
            "imagesize==1.4.1\n",
            "imbalanced-learn==0.13.0\n",
            "immutabledict==4.2.1\n",
            "importlib_metadata==8.7.0\n",
            "importlib_resources==6.5.2\n",
            "imutils==0.5.4\n",
            "inflect==7.5.0\n",
            "iniconfig==2.1.0\n",
            "intel-cmplr-lib-ur==2025.1.0\n",
            "intel-openmp==2025.1.0\n",
            "ipyevents==2.0.2\n",
            "ipyfilechooser==0.6.0\n",
            "ipykernel==6.17.1\n",
            "ipyleaflet==0.19.2\n",
            "ipyparallel==8.8.0\n",
            "ipython==7.34.0\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.5.0\n",
            "ipytree==0.2.2\n",
            "ipywidgets==7.7.1\n",
            "itsdangerous==2.2.0\n",
            "jaraco.classes==3.4.0\n",
            "jaraco.context==6.0.1\n",
            "jaraco.functools==4.1.0\n",
            "jax==0.5.2\n",
            "jax-cuda12-pjrt==0.5.1\n",
            "jax-cuda12-plugin==0.5.1\n",
            "jaxlib==0.5.1\n",
            "jeepney==0.9.0\n",
            "jieba==0.42.1\n",
            "Jinja2==3.1.6\n",
            "jiter==0.9.0\n",
            "joblib==1.4.2\n",
            "jsonpatch==1.33\n",
            "jsonpickle==4.0.5\n",
            "jsonpointer==3.0.0\n",
            "jsonschema==4.23.0\n",
            "jsonschema-specifications==2025.4.1\n",
            "jupyter-client==6.1.12\n",
            "jupyter-console==6.1.0\n",
            "jupyter-leaflet==0.19.2\n",
            "jupyter-server==1.16.0\n",
            "jupyter_core==5.7.2\n",
            "jupyter_kernel_gateway @ git+https://github.com/googlecolab/kernel_gateway@b134e9945df25c2dcb98ade9129399be10788671\n",
            "jupyterlab_pygments==0.3.0\n",
            "jupyterlab_widgets==3.0.14\n",
            "kaggle==1.7.4.2\n",
            "kagglehub==0.3.12\n",
            "keras==3.8.0\n",
            "keras-hub==0.18.1\n",
            "keras-nlp==0.18.1\n",
            "keyring==25.6.0\n",
            "keyrings.google-artifactregistry-auth==1.1.2\n",
            "kiwisolver==1.4.8\n",
            "langchain==0.3.24\n",
            "langchain-core==0.3.56\n",
            "langchain-text-splitters==0.3.8\n",
            "langcodes==3.5.0\n",
            "langsmith==0.3.38\n",
            "language_data==1.3.0\n",
            "launchpadlib==1.10.16\n",
            "lazr.restfulclient==0.14.4\n",
            "lazr.uri==1.0.6\n",
            "lazy_loader==0.4\n",
            "libclang==18.1.1\n",
            "libcudf-cu12 @ https://pypi.nvidia.com/libcudf-cu12/libcudf_cu12-25.2.1-py3-none-manylinux_2_28_x86_64.whl\n",
            "libcugraph-cu12==25.2.0\n",
            "libcuml-cu12==25.2.1\n",
            "libcuvs-cu12==25.2.1\n",
            "libkvikio-cu12==25.2.1\n",
            "libraft-cu12==25.2.0\n",
            "librosa==0.11.0\n",
            "libucx-cu12==1.18.0.post1\n",
            "libucxx-cu12==0.42.0\n",
            "lightgbm @ file:///tmp/lightgbm/LightGBM/dist/lightgbm-4.5.0-py3-none-linux_x86_64.whl\n",
            "linkify-it-py==2.0.3\n",
            "llvmlite==0.43.0\n",
            "locket==1.0.0\n",
            "logical-unification==0.4.6\n",
            "lxml==5.4.0\n",
            "Mako==1.1.3\n",
            "marisa-trie==1.2.1\n",
            "Markdown==3.8\n",
            "markdown-it-py==3.0.0\n",
            "MarkupSafe==3.0.2\n",
            "matplotlib==3.10.0\n",
            "matplotlib-inline==0.1.7\n",
            "matplotlib-venn==1.1.2\n",
            "mdit-py-plugins==0.4.2\n",
            "mdurl==0.1.2\n",
            "miniKanren==1.0.3\n",
            "missingno==0.5.2\n",
            "mistune==3.1.3\n",
            "mizani==0.13.3\n",
            "mkl==2025.0.1\n",
            "ml-dtypes==0.4.1\n",
            "mlxtend==0.23.4\n",
            "more-itertools==10.7.0\n",
            "moviepy==1.0.3\n",
            "mpmath==1.3.0\n",
            "msgpack==1.1.0\n",
            "multidict==6.4.3\n",
            "multipledispatch==1.0.0\n",
            "multitasking==0.0.11\n",
            "murmurhash==1.0.12\n",
            "music21==9.3.0\n",
            "namex==0.0.9\n",
            "narwhals==1.37.1\n",
            "natsort==8.4.0\n",
            "nbclassic==1.3.0\n",
            "nbclient==0.10.2\n",
            "nbconvert==7.16.6\n",
            "nbformat==5.10.4\n",
            "ndindex==1.9.2\n",
            "nest-asyncio==1.6.0\n",
            "networkx==3.4.2\n",
            "nibabel==5.3.2\n",
            "nltk==3.9.1\n",
            "notebook==6.5.7\n",
            "notebook_shim==0.2.4\n",
            "numba==0.60.0\n",
            "numba-cuda==0.2.0\n",
            "numexpr==2.10.2\n",
            "numpy==2.0.2\n",
            "nvidia-cublas-cu12==12.1.3.1\n",
            "nvidia-cuda-cupti-cu12==12.1.105\n",
            "nvidia-cuda-nvcc-cu12==12.5.82\n",
            "nvidia-cuda-nvrtc-cu12==12.1.105\n",
            "nvidia-cuda-runtime-cu12==12.1.105\n",
            "nvidia-cudnn-cu12==8.9.2.26\n",
            "nvidia-cufft-cu12==11.0.2.54\n",
            "nvidia-curand-cu12==10.3.2.106\n",
            "nvidia-cusolver-cu12==11.4.5.107\n",
            "nvidia-cusparse-cu12==12.1.0.106\n",
            "nvidia-cusparselt-cu12==0.6.2\n",
            "nvidia-ml-py==12.570.86\n",
            "nvidia-nccl-cu12==2.20.5\n",
            "nvidia-nvcomp-cu12==4.2.0.11\n",
            "nvidia-nvjitlink-cu12==12.5.82\n",
            "nvidia-nvtx-cu12==12.1.105\n",
            "nvtx==0.2.11\n",
            "nx-cugraph-cu12 @ https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-25.2.0-py3-none-any.whl\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.2.2\n",
            "openai==1.76.0\n",
            "opencv-contrib-python==4.11.0.86\n",
            "opencv-python==4.11.0.86\n",
            "opencv-python-headless==4.11.0.86\n",
            "openpyxl==3.1.5\n",
            "opentelemetry-api==1.16.0\n",
            "opentelemetry-sdk==1.16.0\n",
            "opentelemetry-semantic-conventions==0.37b0\n",
            "opt_einsum==3.4.0\n",
            "optax==0.2.4\n",
            "optree==0.15.0\n",
            "orbax-checkpoint==0.11.12\n",
            "orjson==3.10.17\n",
            "osqp==1.0.3\n",
            "packaging==24.2\n",
            "pandas==2.2.2\n",
            "pandas-datareader==0.10.0\n",
            "pandas-gbq==0.28.0\n",
            "pandas-stubs==2.2.2.240909\n",
            "pandocfilters==1.5.1\n",
            "panel==1.6.3\n",
            "param==2.2.0\n",
            "parso==0.8.4\n",
            "parsy==2.1\n",
            "partd==1.4.2\n",
            "pathlib==1.0.1\n",
            "patsy==1.0.1\n",
            "peewee==3.18.0\n",
            "peft==0.15.2\n",
            "pexpect==4.9.0\n",
            "pickleshare==0.7.5\n",
            "pillow==11.2.1\n",
            "platformdirs==4.3.7\n",
            "plotly==5.24.1\n",
            "plotnine==0.14.5\n",
            "pluggy==1.5.0\n",
            "ply==3.11\n",
            "polars==1.21.0\n",
            "pooch==1.8.2\n",
            "portpicker==1.5.2\n",
            "preshed==3.0.9\n",
            "prettytable==3.16.0\n",
            "proglog==0.1.11\n",
            "progressbar2==4.5.0\n",
            "prometheus_client==0.21.1\n",
            "promise==2.3\n",
            "prompt_toolkit==3.0.51\n",
            "propcache==0.3.1\n",
            "prophet==1.1.6\n",
            "proto-plus==1.26.1\n",
            "protobuf==5.29.4\n",
            "psutil==5.9.5\n",
            "psycopg2==2.9.10\n",
            "ptyprocess==0.7.0\n",
            "py-cpuinfo==9.0.0\n",
            "py4j==0.10.9.7\n",
            "pyarrow==18.1.0\n",
            "pyasn1==0.6.1\n",
            "pyasn1_modules==0.4.2\n",
            "pycairo==1.28.0\n",
            "pycocotools==2.0.8\n",
            "pycparser==2.22\n",
            "pydantic==2.11.3\n",
            "pydantic_core==2.33.1\n",
            "pydata-google-auth==1.9.1\n",
            "pydot==3.0.4\n",
            "pydotplus==2.0.2\n",
            "PyDrive==1.3.1\n",
            "PyDrive2==1.21.3\n",
            "pyerfa==2.0.1.5\n",
            "pygame==2.6.1\n",
            "pygit2==1.18.0\n",
            "Pygments==2.19.1\n",
            "PyGObject==3.42.0\n",
            "PyJWT==2.10.1\n",
            "pylibcudf-cu12 @ https://pypi.nvidia.com/pylibcudf-cu12/pylibcudf_cu12-25.2.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\n",
            "pylibcugraph-cu12==25.2.0\n",
            "pylibraft-cu12==25.2.0\n",
            "pymc==5.22.0\n",
            "pymystem3==0.2.0\n",
            "pynndescent==0.5.13\n",
            "pynvjitlink-cu12==0.5.2\n",
            "pynvml==12.0.0\n",
            "pyogrio==0.10.0\n",
            "pyomo==6.9.2\n",
            "PyOpenGL==3.1.9\n",
            "pyOpenSSL==24.2.1\n",
            "pyparsing==3.2.3\n",
            "pyperclip==1.9.0\n",
            "pyproj==3.7.1\n",
            "pyproject_hooks==1.2.0\n",
            "pyshp==2.3.1\n",
            "PySocks==1.7.1\n",
            "pyspark==3.5.5\n",
            "pytensor==2.30.3\n",
            "pytest==8.3.5\n",
            "python-apt==0.0.0\n",
            "python-box==7.3.2\n",
            "python-dateutil==2.9.0.post0\n",
            "python-louvain==0.16\n",
            "python-slugify==8.0.4\n",
            "python-snappy==0.7.3\n",
            "python-utils==3.9.1\n",
            "pytz==2025.2\n",
            "pyviz_comms==3.0.4\n",
            "PyYAML==6.0.2\n",
            "pyzmq==24.0.1\n",
            "raft-dask-cu12==25.2.0\n",
            "rapids-dask-dependency==25.2.0\n",
            "ratelim==0.1.6\n",
            "referencing==0.36.2\n",
            "regex==2024.11.6\n",
            "requests==2.32.3\n",
            "requests-oauthlib==2.0.0\n",
            "requests-toolbelt==1.0.0\n",
            "requirements-parser==0.9.0\n",
            "rich==13.9.4\n",
            "rmm-cu12==25.2.0\n",
            "roman-numerals-py==3.1.0\n",
            "rpds-py==0.24.0\n",
            "rpy2==3.5.17\n",
            "rsa==4.9.1\n",
            "safetensors==0.5.3\n",
            "scikit-image==0.25.2\n",
            "scikit-learn==1.6.1\n",
            "scipy==1.15.2\n",
            "scooby==0.10.1\n",
            "scs==3.2.7.post2\n",
            "seaborn==0.13.2\n",
            "SecretStorage==3.3.3\n",
            "Send2Trash==1.8.3\n",
            "sentence-transformers==3.4.1\n",
            "sentencepiece==0.2.0\n",
            "sentry-sdk==2.27.0\n",
            "setproctitle==1.3.6\n",
            "shap==0.47.2\n",
            "shapely==2.1.0\n",
            "shellingham==1.5.4\n",
            "simple-parsing==0.1.7\n",
            "simplejson==3.20.1\n",
            "simsimd==6.2.1\n",
            "six==1.17.0\n",
            "sklearn-compat==0.1.3\n",
            "sklearn-pandas==2.2.0\n",
            "slicer==0.0.8\n",
            "smart-open==7.1.0\n",
            "smmap==5.0.2\n",
            "sniffio==1.3.1\n",
            "snowballstemmer==2.2.0\n",
            "sortedcontainers==2.4.0\n",
            "soundfile==0.13.1\n",
            "soupsieve==2.7\n",
            "soxr==0.5.0.post1\n",
            "spacy==3.8.5\n",
            "spacy-legacy==3.0.12\n",
            "spacy-loggers==1.0.5\n",
            "spanner-graph-notebook==1.1.6\n",
            "Sphinx==8.2.3\n",
            "sphinxcontrib-applehelp==2.0.0\n",
            "sphinxcontrib-devhelp==2.0.0\n",
            "sphinxcontrib-htmlhelp==2.1.0\n",
            "sphinxcontrib-jsmath==1.0.1\n",
            "sphinxcontrib-qthelp==2.0.0\n",
            "sphinxcontrib-serializinghtml==2.0.0\n",
            "SQLAlchemy==2.0.40\n",
            "sqlglot==25.20.2\n",
            "sqlparse==0.5.3\n",
            "srsly==2.5.1\n",
            "stanio==0.5.1\n",
            "statsmodels==0.14.4\n",
            "stringzilla==3.12.5\n",
            "sympy==1.13.1\n",
            "tables==3.10.2\n",
            "tabulate==0.9.0\n",
            "tbb==2022.1.0\n",
            "tblib==3.1.0\n",
            "tcmlib==1.3.0\n",
            "tenacity==9.1.2\n",
            "tensorboard==2.18.0\n",
            "tensorboard-data-server==0.7.2\n",
            "tensorflow==2.18.0\n",
            "tensorflow-datasets==4.9.8\n",
            "tensorflow-hub==0.16.1\n",
            "tensorflow-io-gcs-filesystem==0.37.1\n",
            "tensorflow-metadata==1.17.1\n",
            "tensorflow-probability==0.25.0\n",
            "tensorflow-text==2.18.1\n",
            "tensorflow_decision_forests==1.11.0\n",
            "tensorstore==0.1.74\n",
            "termcolor==3.0.1\n",
            "terminado==0.18.1\n",
            "text-unidecode==1.3\n",
            "textblob==0.19.0\n",
            "tf-slim==1.1.0\n",
            "tf_keras==2.18.0\n",
            "thinc==8.3.6\n",
            "threadpoolctl==3.6.0\n",
            "tifffile==2025.3.30\n",
            "timm==1.0.15\n",
            "tinycss2==1.4.0\n",
            "tokenizers==0.21.1\n",
            "toml==0.10.2\n",
            "toolz==0.12.1\n",
            "torch==2.3.0\n",
            "torchaudio @ https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.18.0\n",
            "torchvision @ https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl\n",
            "tornado==6.4.2\n",
            "tqdm==4.67.1\n",
            "traitlets==5.7.1\n",
            "traittypes==0.2.1\n",
            "transformers==4.51.3\n",
            "treelite==4.4.1\n",
            "treescope==0.1.9\n",
            "triton==2.3.0\n",
            "tweepy==4.15.0\n",
            "typeguard==4.4.2\n",
            "typer==0.15.3\n",
            "types-pytz==2025.2.0.20250326\n",
            "types-setuptools==80.0.0.20250429\n",
            "typing-inspection==0.4.0\n",
            "typing_extensions==4.13.2\n",
            "tzdata==2025.2\n",
            "tzlocal==5.3.1\n",
            "uc-micro-py==1.0.3\n",
            "ucx-py-cu12==0.42.0\n",
            "ucxx-cu12==0.42.0\n",
            "umap-learn==0.5.7\n",
            "umf==0.10.0\n",
            "uritemplate==4.1.1\n",
            "urllib3==2.4.0\n",
            "vega-datasets==0.9.0\n",
            "wadllib==1.3.6\n",
            "wandb==0.19.10\n",
            "wasabi==1.1.3\n",
            "wcwidth==0.2.13\n",
            "weasel==0.4.1\n",
            "webcolors==24.11.1\n",
            "webencodings==0.5.1\n",
            "websocket-client==1.8.0\n",
            "websockets==15.0.1\n",
            "Werkzeug==3.1.3\n",
            "widgetsnbextension==3.6.10\n",
            "wordcloud==1.9.4\n",
            "wrapt==1.17.2\n",
            "wurlitzer==3.1.1\n",
            "xarray==2025.3.1\n",
            "xarray-einstats==0.8.0\n",
            "xgboost==2.1.4\n",
            "xlrd==2.0.1\n",
            "xyzservices==2025.4.0\n",
            "yarl==1.20.0\n",
            "ydf==0.11.0\n",
            "yellowbrick==1.5\n",
            "yfinance==0.2.57\n",
            "zict==3.0.0\n",
            "zipp==3.21.0\n",
            "zstandard==0.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jXnagBYmi_YI"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "dir(torchtext)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-QhBRQ0JEwO",
        "outputId": "582835f5-cea6-4b95-de98-e7edc5be9613"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_CACHE_DIR',\n",
              " '_TEXT_BUCKET',\n",
              " '_TORCHTEXT_DEPRECATION_MSG',\n",
              " '_WARN',\n",
              " '__all__',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '__version__',\n",
              " '_extension',\n",
              " '_get_torch_home',\n",
              " '_internal',\n",
              " '_torchtext',\n",
              " 'disable_torchtext_deprecation_warning',\n",
              " 'git_version',\n",
              " 'os',\n",
              " 'version']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"../week2_files/Basic-MT_week2_files\"\n",
        "\n",
        "files = {\n",
        "    \"train_en\": \"train.en-de.bpe.en\",\n",
        "    \"train_de\": \"train.en-de.bpe.de\",\n",
        "    \"dev_en\":   \"dev.en-de.bpe.en\",\n",
        "    \"dev_de\":   \"dev.en-de.bpe.de\",\n",
        "    \"test_en\":  \"test.en-de.bpe.en\",\n",
        "    \"test_de\":  \"test.en-de.bpe.de\",\n",
        "}\n",
        "\n",
        "data = {}\n",
        "\n",
        "for key, filename in files.items():\n",
        "    with open(f\"{data_path}/{filename}\", encoding=\"utf-8\") as f:\n",
        "        data[key] = f.read().splitlines()"
      ],
      "metadata": {
        "id": "lFDXjJpt1Mo_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[\"train_en\"][:3])     #first 3 lines of English training data\n",
        "print(data[\"dev_de\"][:5])       #first 5 lines of German dev data"
      ],
      "metadata": {
        "id": "bvQO-u6g1xB3",
        "outputId": "01cf31ee-cf9d-457b-8feb-36d2a50baf87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A recent analysis by Ap@@ al@@ de@@ tt@@ i et al. (201@@ 1) suggest@@ s that G@@ ong@@ x@@ ian@@ os@@ aur@@ us was more bas@@ al than V@@ ul@@ can@@ od@@ on, T@@ az@@ ou@@ d@@ as@@ aur@@ us and Is@@ an@@ os@@ aur@@ us, but more der@@ i@@ ved than the early s@@ aur@@ op@@ o@@ ds An@@ t@@ et@@ on@@ it@@ r@@ us, L@@ ess@@ em@@ s@@ aur@@ us, B@@ lik@@ an@@ as@@ aur@@ us, Cam@@ el@@ o@@ ti@@ a and Mel@@ an@@ or@@ os@@ aur@@ us.', 'R@@ ei@@ ch@@ h@@ art also carried out execu@@ tions in C@@ olog@@ ne, Fran@@ k@@ fur@@ t-@@ Pre@@ ung@@ es@@ hei@@ m, Berlin@@ -@@ Pl@@ ö@@ tz@@ en@@ se@@ e, B@@ ran@@ den@@ burg@@ -@@ G@@ ör@@ den and B@@ res@@ lau@@ , where central execu@@ tion sit@@ es had also been construc@@ ted.', 'U@@ ph@@ old the right of all, without disc@@ ri@@ min@@ ation, to a natural and social en@@ vir@@ on@@ ment suppor@@ tive of human di@@ gn@@ ity, bo@@ di@@ ly health and spir@@ itu@@ al w@@ ell@@ -@@ b@@ ein@@ g, with special attention to the rights of in@@ di@@ gen@@ ous pe@@ op@@ les and min@@ or@@ ities.']\n",
            "['Bes@@ on@@ der@@ s bekannt wurden ihre Auf@@ nahmen von einem F@@ est 193@@ 5, zu dem G@@ ä@@ ste als g@@ rie@@ ch@@ ische G@@ öt@@ ter und G@@ ö@@ tt@@ innen ver@@ klei@@ det kam@@ en.', 'Ter@@ é@@ zi@@ a Mor@@ a arbeitet an einer Tri@@ log@@ ie um den I@@ T-@@ Spezi@@ alisten D@@ ari@@ us K@@ opp@@ , von der Band I „@@ Der einzige Mann auf dem Kontin@@ ent@@ “ und Band II „@@ Das Un@@ ge@@ heu@@ er@@ “ bereits erschienen sind.', 'Eine erste Be@@ fahr@@ ung dieses Ab@@ schnit@@ ts gelang G@@ ün@@ ther J. Wol@@ f mit sieben Teil@@ nehm@@ ern seines Eis@@ kur@@ s@@ es.', 'Sie wurden 1970 in 100 9@@ 0@@ 3 und 9@@ 0@@ 4, 1973 in 19@@ 9 00@@ 3 und 00@@ 4 um@@ gen@@ um@@ mer@@ t.', 'Bei dem Gra@@ b handelt es sich wahrscheinlich um eine gest@@ ör@@ te An@@ la@@ ge, die früher mit Hol@@ z oder St@@ einen abge@@ deck@@ t war.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gjVU2ynri_YJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d18dcd1-4a14-40d2-f0c7-8bea0f06f47d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['T_destination', '__annotations__', '__call__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__jit_unused_properties__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__prepare_scriptable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'append_token', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_default_index', 'get_extra_state', 'get_itos', 'get_parameter', 'get_stoi', 'get_submodule', 'half', 'insert_token', 'ipu', 'is_jitable', 'load_state_dict', 'lookup_indices', 'lookup_token', 'lookup_tokens', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'set_default_index', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'vocab', 'xpu', 'zero_grad']\n"
          ]
        }
      ],
      "source": [
        "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import vocab\n",
        "#from torchtext.utils import download_from_url, extract_archive\n",
        "import io\n",
        "\n",
        "#0=en 1=de\n",
        "#soure and target data\n",
        "#NOTE: USE clean bpe data!\n",
        "\n",
        "train_filepaths = [f\"{data_path}/train.en-de.bpe.en\", f\"{data_path}/train.en-de.bpe.de\"]\n",
        "val_filepaths   = [f\"{data_path}/dev.en-de.bpe.en\",   f\"{data_path}/dev.en-de.bpe.de\"]\n",
        "test_filepaths  = [f\"{data_path}/test.en-de.bpe.en\",  f\"{data_path}/test.en-de.bpe.de\"]\n",
        "\n",
        "\n",
        "\n",
        "#NB: lines in input files already consists of bpe tokens!!!\n",
        "#de_tokenizer = get_tokenizer('moses', language='de')\n",
        "de_tokenizer = None\n",
        "#en_tokenizer = get_tokenizer('moses', language='en')\n",
        "en_tokenizer = None\n",
        "\n",
        "\n",
        "def build_vocab(filepath, tokenizer=None):\n",
        "  counter = Counter()\n",
        "  with io.open(filepath, encoding=\"utf8\") as f:\n",
        "    for string_ in f:\n",
        "      #counter.update(tokenizer(string_))\n",
        "      counter.update(string_.split())\n",
        "  return vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "\n",
        "#Vocab\n",
        "en_vocab = build_vocab(train_filepaths[0], en_tokenizer)\n",
        "de_vocab = build_vocab(train_filepaths[1], de_tokenizer)\n",
        "\n",
        "print(dir(en_vocab))\n",
        "en_vocab.set_default_index(en_vocab['<unk>'])\n",
        "de_vocab.set_default_index(de_vocab['<unk>'])\n",
        "\n",
        "\n",
        "def data_process(filepaths):\n",
        "  raw_en_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "  raw_de_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "  data = []\n",
        "  for (raw_en, raw_de) in zip(raw_en_iter, raw_de_iter):\n",
        "    en_tensor_ = torch.tensor([en_vocab[token] for token in raw_en.split()], #en_tokenizer(raw_en)\n",
        "                            dtype=torch.long)\n",
        "    de_tensor_ = torch.tensor([de_vocab[token] for token in raw_de.split()], #de_tokenizer(raw_de)\n",
        "                            dtype=torch.long)\n",
        "    data.append((en_tensor_, de_tensor_))\n",
        "  return data\n",
        "\n",
        "#pre-process\n",
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data))\n",
        "print(len(val_data))\n",
        "print(len(test_data))"
      ],
      "metadata": {
        "id": "ooMZXrnTzj_9",
        "outputId": "1046d881-6d9c-49e5-bbff-6556d3e9fde2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "466\n",
            "467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Wo6Mz24Itw7B"
      },
      "outputs": [],
      "source": [
        "#NOTE: if you are low on credits or testing only use a piece of the data e.g. 1K segments (for trying; 20k for final training)\n",
        "train_data = train_data[:20000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8FHO7cfuHz_",
        "outputId": "8f805e0d-b6a3-4fcd-fcea-d069113b1c64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "466\n",
            "467\n"
          ]
        }
      ],
      "source": [
        "print(len(train_data))\n",
        "print(len(val_data))\n",
        "print(len(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyZRMk_Yi_YJ"
      },
      "source": [
        "Define the device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNS-Dmk0i_YJ",
        "outputId": "44350f04-030e-4767-d86d-bd6c678214b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUIbF0Wi_YK"
      },
      "source": [
        "Create the iterators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tCHlMlZYi_YK"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 8\n",
        "PAD_IDX = de_vocab['<pad>']\n",
        "BOS_IDX = de_vocab['<bos>']\n",
        "EOS_IDX = de_vocab['<eos>']\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def generate_batch(data_batch):\n",
        "    en_batch, de_batch = [], []\n",
        "    for (en_item, de_item) in data_batch:\n",
        "        de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    de_batch = pad_sequence(de_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "    return en_batch, de_batch\n",
        "\n",
        "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=False, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                       shuffle=False, collate_fn=generate_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjnN1Aj0i_YK"
      },
      "source": [
        "## Building the Seq2Seq Model\n",
        "\n",
        "### Encoder\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zf4mEJnhi_YK"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True, batch_first=True)       #RNN\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)       #linear layer for projecting 'hidden' to enc_hid_dim\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))        #[B, S, E]\n",
        "        outputs, hidden = self.rnn(embedded)                #outputs: [B, S, enc_hid_dim*2]; hidden: [2, B, enc_hid_dim]\n",
        "\n",
        "        h1 = hidden[-2, :, :]       #forward\n",
        "        h2 = hidden[-1, :, :]       #backward\n",
        "        h_cat = torch.cat((h1, h2), dim=1)                  # [B, enc_hid_dim*2]\n",
        "        hidden = torch.tanh(self.fc(h_cat))                 # [B, dec_hid_dim]\n",
        "\n",
        "        return outputs, hidden\n",
        "\n",
        "#NB:\n",
        "#B = Batch size\n",
        "#S = Source sequence length (number of tokens in the input sentence)\n",
        "#T = Target sequence length (number of tokens in the output sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention\n",
        "\n",
        "## Luong Attention\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5Av3fvEwEAXM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kP4jL01Vi_YL"
      },
      "outputs": [],
      "source": [
        "class LuongAttention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):     #keys, query\n",
        "        \"\"\"\n",
        "        Compute Luong-style (concat) attention weights.\n",
        "\n",
        "        Args:\n",
        "            hidden (Tensor): [B, dec_hid_dim] --> current decoder hidden state\n",
        "            encoder_outputs (Tensor): [B, S, enc_hid_dim*2] --> encoder outputs\n",
        "\n",
        "        Returns:\n",
        "            attention_weights (Tensor): [B, S] --> normalized attention scores\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "        src_len = encoder_outputs.shape[1]\n",
        "\n",
        "        #repeat decoder hidden state to match encoder output length\n",
        "        #hidden: [B, 1, dec_hid_dim] --> [B, S, dec_hid_dim]\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        #concatenate decoder hidden state with encoder outputs along the last dimension\n",
        "        #shape: [B, S, (enc_hid_dim*2 + dec_hid_dim)]\n",
        "        scores = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2)))\n",
        "\n",
        "\n",
        "        #compute raw attention scores and remove last dimension\n",
        "        #scores: [B, S, dec_hid_dim] --> attention: [B, S]\n",
        "        attention = self.v(scores).squeeze(2)\n",
        "\n",
        "        #normalize scores over source sequence length\n",
        "        return F.softmax(attention, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sFSa__e8_ZQB"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "\n",
        "        self.Wa = nn.Linear(dec_hid_dim, dec_hid_dim, bias=False)\n",
        "        self.Ua = nn.Linear(enc_hid_dim * 2, dec_hid_dim, bias=False)\n",
        "        self.Va = nn.Linear(dec_hid_dim, 1, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):     #keys, query\n",
        "        \"\"\"\n",
        "        Compute Bahdanau-style (additive) attention weights.\n",
        "\n",
        "        Args:\n",
        "            hidden (Tensor): [B, dec_hid_dim] --> current decoder hidden state\n",
        "            encoder_outputs (Tensor): [B, S, enc_hid_dim*2] --> encoder outputs\n",
        "\n",
        "        Returns:\n",
        "            weights (Tensor): [B, S] --> normalized attention scores\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "        src_len = encoder_outputs.shape[1]\n",
        "\n",
        "        #repeat decoder hidden state across src_len\n",
        "        #hidden: [B, 1, dec_hid_dim] --> [B, S, dec_hid_dim]\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        #apply additive attention: Va(tanh(Wa(hidden) + Ua(encoder_outputs)))\n",
        "        scores = self.Va(torch.tanh(self.Wa(hidden) + self.Ua(encoder_outputs)))        #[B, S, 1]\n",
        "        scores = scores.squeeze(2)      #[B, S]\n",
        "\n",
        "        #softmax over source sequence length\n",
        "        weights = F.softmax(scores, dim=1)      #[B, S]\n",
        "\n",
        "        return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWvboaMki_YL"
      },
      "source": [
        "### Decoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "t_J_RZPbi_YL"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim, batch_first=True)\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        #attention\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input: [B] --> current target token IDs\n",
        "            hidden: [B, dec_hid_dim] --> previous decoder hidden state\n",
        "            encoder_outputs: [B, S, enc_hid_dim*2] --> all encoder outputs\n",
        "\n",
        "\n",
        "        Returns:\n",
        "            prediction: [B, output_dim] --> logits for next token\n",
        "            hidden: [B, dec_hid_dim] --> new decoder hidden state\n",
        "        \"\"\"\n",
        "\n",
        "        input = input.unsqueeze(0)      #[1, B] add sequence length dimension\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))      #[1, B, emb_dim]\n",
        "\n",
        "        a = self.attention(hidden, encoder_outputs)     #[B, S] attention weights\n",
        "\n",
        "        a = a.unsqueeze(1)      #[B, 1, S] for batch matrix multiplication\n",
        "\n",
        "        #encoder_outputs: [B, S, enc_hid_dim*2]\n",
        "        weighted = torch.bmm(a, encoder_outputs)        #[B, 1, enc_hid_dim*2] context vector\n",
        "\n",
        "        weighted = weighted.permute(1, 0, 2)        #[1, B, enc_hid_dim*2] match RNN input\n",
        "\n",
        "        rnn_input = torch.cat((embedded, weighted), dim=2)      #[1, B, emb_dim + enc_hid_dim*2]\n",
        "\n",
        "        rnn_input = rnn_input.permute(1, 0, 2)      #[B, 1, emb_dim + enc_hid_dim*2]\n",
        "        hidden = hidden.unsqueeze(0)                #[1, B, dec_hid_dim] match GRU expected input\n",
        "\n",
        "        output, hidden = self.rnn(rnn_input, hidden)\n",
        "        #output: [B, 1, dec_hid_dim]\n",
        "        #hidden: [1, B, dec_hid_dim]\n",
        "\n",
        "        embedded = embedded.squeeze(0)      #[B, emb_dim]\n",
        "        output = output.squeeze(1)          #[B, dec_hid_dim]\n",
        "        weighted = weighted.squeeze(0)      #[B, enc_hid_dim*2]\n",
        "\n",
        "        #concatenate GRU output, context vector, and embedding; then predict next token\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))        #[B, output_dim]\n",
        "\n",
        "        return prediction, hidden.squeeze(0)        #return predicted logits and new hidden state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d33lfUX9i_YL"
      },
      "source": [
        "### Seq2Seq\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "GwoMHRFxi_YL"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: [B, S] --> source token indices\n",
        "            trg: [B, T] --> target token indices\n",
        "            teacher_forcing_ratio: float --> probability of using ground truth as the next input\n",
        "\n",
        "        Returns:\n",
        "            outputs: [B, T, output_dim] --> predicted token logits for each time step\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
        "\n",
        "        #run the encoder on the full source sequence\n",
        "        #encoder_outputs: [B, src_len, enc_hid_dim * 2] --> all hidden states of the input sequence\n",
        "        #hidden: [B, dec_hid_dim] --> final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[:,0]\n",
        "\n",
        "        # unroll RNN\n",
        "        for t in range(1, trg_len):\n",
        "            # Decode one token\n",
        "            # input: [B]\n",
        "            # hidden: [B, dec_hid_dim]\n",
        "            # encoder_outputs: [B, src_len, enc_hid_dim*2]\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "\n",
        "            #predictions\n",
        "            outputs[:, t] = output      #[B, output_dim]\n",
        "\n",
        "            #whether to use teacher forcing\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            #greedy search\n",
        "            top1 = output.argmax(1)     #[B]\n",
        "\n",
        "            #if teacher forcing, use gold token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e06yvLvei_YL"
      },
      "source": [
        "## Training the Seq2Seq Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "5jC9nxari_YM"
      },
      "outputs": [],
      "source": [
        "#voc sizes\n",
        "INPUT_DIM = len(en_vocab)\n",
        "OUTPUT_DIM = len(de_vocab)\n",
        "\n",
        "#embedding and hidden dimensions\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.2\n",
        "\n",
        "#attention mechanism (OR)\n",
        "attn = LuongAttention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "\n",
        "#alternative: use Bahdanau attention instead\n",
        "#attn = BahdanauAttention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "\n",
        "#build encoder and decoder\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGLEjaJu2NjY",
        "outputId": "12bae1c4-a581-4f1d-cab8-ec3117b096dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5871\n",
            "6423\n"
          ]
        }
      ],
      "source": [
        "print(len(en_vocab))        #BPE size 16k approx --> no?\n",
        "print(len(de_vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUD45Ys5i_YM",
        "outputId": "c266fe00-60bd-4cfa-c2ef-a4d9ed38803d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(5871, 256)\n",
              "    (rnn): GRU(256, 512, batch_first=True, bidirectional=True)\n",
              "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): LuongAttention(\n",
              "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(6423, 256)\n",
              "    (rnn): GRU(1280, 512, batch_first=True)\n",
              "    (fc_out): Linear(in_features=1792, out_features=6423, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "    (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
              "    (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmY0twjqi_YM",
        "outputId": "81a1ecd4-c40b-40ea-9a05-1732954cbe8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 21,884,439 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6M5jMx9i_YM"
      },
      "source": [
        "We create an optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "GkFpvqEii_YM"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POJ6uKGei_YM"
      },
      "source": [
        "We initialize the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "McD_2eGsi_YM"
      },
      "outputs": [],
      "source": [
        "TRG_PAD_IDX = de_vocab['<pad>']     #index of <pad> token in target voc\n",
        "\n",
        "#https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "#CrossEntropyLoss expects raw logits (not softmaxed) and ignores padding tokens in loss computation\n",
        "#it will compute loss only over non-pad positions\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBn2uizpi_YM",
        "outputId": "30e88884-2a3e-4e0a-e8e8-6fc8c1dd4f99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "print(TRG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ZWBC8pDAi_YN"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    #set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for (src, trg) in tqdm(iterator):\n",
        "        #move source and target tensors to device\n",
        "        src, trg = src.to(model.device), trg.to(model.device)\n",
        "\n",
        "        #zero the gradients from the previous batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #forward pass through the model\n",
        "        output = model(src, trg)\n",
        "\n",
        "        #output: [B, trg_len, output_dim]\n",
        "        #trg:    [B, trg_len]\n",
        "\n",
        "        output = output.permute(1, 0, 2)        #[trg_len, B, output_dim]\n",
        "        trg = trg.permute(1, 0)                 #[trg_len, B]\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        #skip the first token (<sos>) in both prediction and target\n",
        "        output = output[1:].reshape(-1, output_dim)     #[(trg_len-1)*B, output_dim]\n",
        "        trg = trg[1:].reshape(-1)                       #[(trg_len-1)*B]\n",
        "\n",
        "        #compute the loss (ignoring padding index)\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        #backpropagate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        #clip gradients to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        #update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    #return average loss per batch\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "lJGxS6mFi_YN"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    #set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (src, trg) in iterator:\n",
        "            #move data to device again\n",
        "            src, trg = src.to(model.device), trg.to(model.device)\n",
        "\n",
        "            #forward pass with no teacher forcing\n",
        "            output = model(src, trg, teacher_forcing_ratio=0)       #NB: why turn off here?\n",
        "\n",
        "            #trg: [B, trg_len]\n",
        "            #output: [B, trg_len, output_dim]\n",
        "\n",
        "            output = output.permute(1, 0, 2)        #[trg_len, B, output_dim]\n",
        "            trg = trg.permute(1, 0)                 #[trg_len, B]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output[1:].reshape(-1, output_dim)     #[(trg_len-1)*B, output_dim]\n",
        "            trg = trg[1:].reshape(-1)                       #[(trg_len-1)*B]\n",
        "\n",
        "            #compute loss without backpropagation\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def compute_bleu(model, iterator, vocab, device):\n",
        "    model.eval()\n",
        "    trgs = []\n",
        "    preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg in iterator:\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            output = model(src, trg, teacher_forcing_ratio=0)       #greedy decoding\n",
        "            output = output.argmax(2)       #[B, trg_len]\n",
        "\n",
        "            for i in range(trg.shape[0]):\n",
        "                #skip <sos>, stop at <eos>\n",
        "                trg_tokens = [vocab.lookup_token(tok.item()) for tok in trg[i, 1:] if tok.item() != vocab['<pad>']]\n",
        "                pred_tokens = [vocab.lookup_token(tok.item()) for tok in output[i, 1:] if tok.item() != vocab['<pad>']]\n",
        "\n",
        "                #ensure at least one token is present (BLEU requires non-empty candidates and refs)\n",
        "                if len(pred_tokens) > 0 and len(trg_tokens) > 0:\n",
        "                    preds.append(pred_tokens)\n",
        "                    trgs.append([trg_tokens])       #BLEU expects list of references\n",
        "\n",
        "    return bleu_score(preds, trgs) * 100        #BLEU in percentage"
      ],
      "metadata": {
        "id": "hxdAj8dxpxJh"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFGhRjIMi_YN",
        "outputId": "f09067df-38e1-4e8d-eac3-23aa711ee845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 7.701\tTrain PPL: 2209.549\n",
            "\t Validation Loss: 7.457\tValidation PPL: 1731.264\n",
            "\tValidation BLEU: 0.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  4.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02\n",
            "\tTrain Loss: 7.218\tTrain PPL: 1363.552\n",
            "\t Validation Loss: 7.438\tValidation PPL: 1699.539\n",
            "\tValidation BLEU: 0.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03\n",
            "\tTrain Loss: 7.064\tTrain PPL: 1169.537\n",
            "\t Validation Loss: 7.545\tValidation PPL: 1891.274\n",
            "\tValidation BLEU: 0.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04\n",
            "\tTrain Loss: 6.865\tTrain PPL: 958.539\n",
            "\t Validation Loss: 7.504\tValidation PPL: 1814.769\n",
            "\tValidation BLEU: 0.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [00:31<00:00,  3.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05\n",
            "\tTrain Loss: 6.640\tTrain PPL: 764.934\n",
            "\t Validation Loss: 7.551\tValidation PPL: 1902.677\n",
            "\tValidation BLEU: 0.00\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 5\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "best_bleu = 0.0\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iter, criterion)\n",
        "\n",
        "    bleu = compute_bleu(model, valid_iter, de_vocab, device)\n",
        "\n",
        "\n",
        "    if valid_loss < best_valid_loss or bleu > best_bleu:\n",
        "        best_valid_loss = valid_loss\n",
        "        best_bleu = bleu\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}\\tTrain PPL: {np.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Validation Loss: {valid_loss:.3f}\\tValidation PPL: {np.exp(valid_loss):7.3f}')\n",
        "    print(f'\\tValidation BLEU: {bleu:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "meHcb9oOi_YN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aba6a7b-7daf-431d-c0b6-b9ef48badfab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tTest Loss: 7.427\tTest PPL: 1681.099\n",
            "\tTest BLEU: 0.00\n"
          ]
        }
      ],
      "source": [
        "#NOTE: load model from file\n",
        "model.load_state_dict(torch.load('model.pt', map_location=device))\n",
        "\n",
        "test_loss = evaluate(model, test_iter, criterion)\n",
        "bleu = compute_bleu(model, test_iter, de_vocab, device)\n",
        "\n",
        "print(f'\\tTest Loss: {test_loss:.3f}\\tTest PPL: {np.exp(test_loss):7.3f}')\n",
        "print(f'\\tTest BLEU: {bleu:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NB: bad results:\n",
        "#solution:\n",
        "    #increase train_data = train_data[:20000]"
      ],
      "metadata": {
        "id": "J0HEYVm0u1lG"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "g7dql_q_i_YN"
      },
      "outputs": [],
      "source": [
        "#clean mem\n",
        "del model\n",
        "del train_iter\n",
        "del valid_iter\n",
        "del test_iter\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}